#!/usr/bin/env python3
"""
Analyze quality of experiences in Replay Buffer
"""
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import json

class ReplayBufferAnalyzer:
    """Analyze quality of experiences in replay buffer"""
    
    def __init__(self, replay_buffer):
        self.buffer = replay_buffer
        
    def analyze_quality(self):
        """Comprehensive analysis of experience quality in buffer"""
        print("="*80)
        print("REPLAY BUFFER QUALITY ANALYSIS")
        print("="*80)
        
        # 1. Basic statistics
        self.analyze_basic_stats()
        
        # 2. Reward distribution
        self.analyze_reward_distribution()
        
        # 3. State diversity
        self.analyze_state_diversity()
        
        # 4. Learning progress
        self.analyze_learning_progress()
        
        # 5. Anomaly detection
        self.detect_anomalies()
        
        # 6. Quality scoring
        self.calculate_quality_score()
    
    def analyze_basic_stats(self):
        """Basic statistics"""
        print("\nğŸ“Š 1. Basic Statistics")
        print("-"*80)
        
        size = self.buffer.size
        max_size = self.buffer.max_size
        
        print(f"Bufferä½¿ç”¨ç‡:     {size:,} / {max_size:,} ({size/max_size*100:.1f}%)")
        
        if size == 0:
            print("âš ï¸  Bufferä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
            return
        
        # Analyze rewards
        rewards = self.buffer.reward[:size].flatten()
        
        print(f"\nRewardç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼:         {rewards.mean():.4f}")
        print(f"  ä¸­ä½æ•°:         {np.median(rewards):.4f}")
        print(f"  æ ‡å‡†å·®:         {rewards.std():.4f}")
        print(f"  æœ€å°å€¼:         {rewards.min():.4f}")
        print(f"  Maximum:        {rewards.max():.4f}")
        
        # Analyze done flags
        dones = self.buffer.not_done[:size].flatten()
        done_count = np.sum(dones == 0)  # not_done=0 means done=True
        
        print(f"\nå®ŒæˆçŠ¶æ€:")
        print(f"  è¾¾åˆ°ç›®æ ‡çš„ç»éªŒ: {done_count:,} ({done_count/size*100:.2f}%)")
        print(f"  æœªå®Œæˆçš„ç»éªŒ:   {size-done_count:,} ({(size-done_count)/size*100:.2f}%)")
    
    def analyze_reward_distribution(self):
        """åˆ†ærewardåˆ†å¸ƒ"""
        print("\nğŸ“ˆ 2. Rewardåˆ†å¸ƒåˆ†æ")
        print("-"*80)
        
        size = self.buffer.size
        if size == 0:
            return
        
        rewards = self.buffer.reward[:size].flatten()
        
        # Categorize rewards
        negative = np.sum(rewards < 0)
        zero = np.sum(rewards == 0)
        small_positive = np.sum((rewards > 0) & (rewards < 1))
        medium_positive = np.sum((rewards >= 1) & (rewards < 5))
        large_positive = np.sum(rewards >= 5)
        
        print(f"Reward Distribution:")
        print(f"  Negative rewards (< 0):     {negative:,} ({negative/size*100:.2f}%)")
        print(f"  Zero rewards (= 0):         {zero:,} ({zero/size*100:.2f}%)")
        print(f"  Small positive (0-1):       {small_positive:,} ({small_positive/size*100:.2f}%)")
        print(f"  Medium positive (1-5):      {medium_positive:,} ({medium_positive/size*100:.2f}%)")
        print(f"  Large positive (â‰¥ 5):       {large_positive:,} ({large_positive/size*100:.2f}%)")
        
        # Quality assessment
        print(f"\nQuality Assessment:")
        if zero / size > 0.5:
            print(f"  âš ï¸  Too many zero rewards ({zero/size*100:.1f}%) - may lack learning signal")
        elif small_positive / size > 0.3:
            print(f"  âœ… Sufficient positive rewards ({(small_positive+medium_positive+large_positive)/size*100:.1f}%)")
        
        if large_positive > 0:
            print(f"  âœ… {large_positive:,} high-quality experiences (reward â‰¥ 5)")
        else:
            print(f"  âš ï¸  No high-quality experiences (reward â‰¥ 5)")
        
        # Percentiles
        print(f"\nReward Percentiles:")
        percentiles = [10, 25, 50, 75, 90, 95, 99]
        for p in percentiles:
            val = np.percentile(rewards, p)
            print(f"  {p}%: {val:.4f}")
    
    def analyze_state_diversity(self):
        """Analyze state diversity"""
        print("\nğŸ¨ 3. State Diversity Analysis")
        print("-"*80)
        
        size = self.buffer.size
        if size == 0:
            return
        
        states = self.buffer.state[:size]
        
        # Analyze belief part (first 5 dimensions)
        beliefs = states[:, :5]
        
        print(f"Beliefç»Ÿè®¡ (å‰5ç»´):")
        for i in range(5):
            cluster_beliefs = beliefs[:, i]
            print(f"  Cluster {i}:")
            print(f"    å¹³å‡å€¼: {cluster_beliefs.mean():.6f}")
            print(f"    æ ‡å‡†å·®: {cluster_beliefs.std():.6f}")
            print(f"    èŒƒå›´:   [{cluster_beliefs.min():.6f}, {cluster_beliefs.max():.6f}]")
        
        # Check diversity
        print(f"\nDiversity Assessment:")
        
        # Calculate overall variance of beliefs
        total_variance = beliefs.var(axis=0).sum()
        print(f"  Total variance: {total_variance:.6f}")
        
        if total_variance < 0.001:
            print(f"  âš ï¸  æ–¹å·®å¤ªå° - states lack diversity")
        elif total_variance < 0.01:
            print(f"  âš ï¸  æ–¹å·®è¾ƒå° - stateå¤šæ ·æ€§ä¸è¶³")
        else:
            print(f"  âœ… Reasonable variance - states have good diversity")
        
        # Check for stuck states (unchanged beliefs)
        unique_states = len(np.unique(beliefs, axis=0))
        print(f"\n  Unique state count: {unique_states:,} / {size:,} ({unique_states/size*100:.1f}%)")
        
        if unique_states / size < 0.1:
            print(f"  âš ï¸  å”¯ä¸€stateå æ¯”è¿‡ä½ - å¯èƒ½å­˜åœ¨stuck states")
        else:
            print(f"  âœ… å”¯ä¸€stateå æ¯”åˆç†")
    
    def analyze_learning_progress(self):
        """Analyze learning progress"""
        print("\nğŸ“š 4. å­¦ä¹ è¿›åº¦åˆ†æ")
        print("-"*80)
        
        size = self.buffer.size
        if size == 0:
            return
        
        states = self.buffer.state[:size]
        next_states = self.buffer.next_state[:size]
        rewards = self.buffer.reward[:size].flatten()
        
        # Calculate belief changes
        beliefs = states[:, :5]
        next_beliefs = next_states[:, :5]
        belief_changes = next_beliefs - beliefs
        
        print(f"Beliefå˜åŒ–ç»Ÿè®¡:")
        print(f"  å¹³å‡å˜åŒ–é‡: {np.abs(belief_changes).mean():.6f}")
        print(f"  Maximum change: {np.abs(belief_changes).max():.6f}")
        
        # Count effective learning experiences
        significant_changes = np.sum(np.abs(belief_changes).sum(axis=1) > 0.001)
        print(f"\nEffective learning experiences:")
        print(f"  æœ‰æ˜¾è‘—å˜åŒ–: {significant_changes:,} ({significant_changes/size*100:.2f}%)")
        print(f"  æ— æ˜¾è‘—å˜åŒ–: {size-significant_changes:,} ({(size-significant_changes)/size*100:.2f}%)")
        
        if significant_changes / size < 0.3:
            print(f"  âš ï¸  æœ‰æ•ˆç»éªŒå æ¯”è¿‡ä½ - å­¦ä¹ ä¿¡å·ä¸è¶³")
        else:
            print(f"  âœ… Reasonable proportion of effective experiences")
        
        # Analyze positive progress
        # Assume target is [0.7, 0.07, 0.07, 0.07, 0.08]
        target = np.array([0.7, 0.07, 0.07, 0.07, 0.08])
        
        current_distances = np.abs(beliefs - target).sum(axis=1)
        next_distances = np.abs(next_beliefs - target).sum(axis=1)
        
        improvements = current_distances > next_distances
        improvement_count = np.sum(improvements)
        
        print(f"\næœå‘ç›®æ ‡çš„è¿›æ­¥:")
        print(f"  æ”¹è¿›çš„ç»éªŒ: {improvement_count:,} ({improvement_count/size*100:.2f}%)")
        print(f"  é€€æ­¥çš„ç»éªŒ: {size-improvement_count:,} ({(size-improvement_count)/size*100:.2f}%)")
        
        if improvement_count / size < 0.3:
            print(f"  âš ï¸  æ”¹è¿›ç»éªŒå æ¯”è¿‡ä½ - ç­–ç•¥å¯èƒ½ä¸å¤Ÿå¥½")
        elif improvement_count / size > 0.6:
            print(f"  âœ… æ”¹è¿›ç»éªŒå æ¯”å¾ˆå¥½ - ç­–ç•¥æ­£åœ¨å­¦ä¹ ")
        else:
            print(f"  âœ… æ”¹è¿›ç»éªŒå æ¯”åˆç†")
    
    def detect_anomalies(self):
        """æ£€æµ‹å¼‚å¸¸ç»éªŒ"""
        print("\nğŸ” 5. å¼‚å¸¸æ£€æµ‹")
        print("-"*80)
        
        size = self.buffer.size
        if size == 0:
            return
        
        anomalies = []
        
        # Detect anomalous rewards
        rewards = self.buffer.reward[:size].flatten()
        
        # Use IQR method to detect outliers
        q1, q3 = np.percentile(rewards, [25, 75])
        iqr = q3 - q1
        lower_bound = q1 - 3 * iqr
        upper_bound = q3 + 3 * iqr
        
        reward_outliers = np.sum((rewards < lower_bound) | (rewards > upper_bound))
        
        if reward_outliers > 0:
            anomalies.append(f"å‘ç°{reward_outliers:,}ä¸ªå¼‚å¸¸rewardå€¼")
        
        # Detect NaN or Inf
        states = self.buffer.state[:size]
        next_states = self.buffer.next_state[:size]
        
        nan_states = np.sum(np.isnan(states))
        nan_next_states = np.sum(np.isnan(next_states))
        nan_rewards = np.sum(np.isnan(rewards))
        
        if nan_states > 0:
            anomalies.append(f"å‘ç°{nan_states:,}ä¸ªNaN stateå€¼")
        if nan_next_states > 0:
            anomalies.append(f"å‘ç°{nan_next_states:,}ä¸ªNaN next_stateå€¼")
        if nan_rewards > 0:
            anomalies.append(f"å‘ç°{nan_rewards:,}ä¸ªNaN rewardå€¼")
        
        # Detect beliefs out of range
        beliefs = states[:, :5]
        out_of_range = np.sum((beliefs < 0) | (beliefs > 1))
        
        if out_of_range > 0:
            anomalies.append(f"å‘ç°{out_of_range:,}ä¸ªè¶…å‡º[0,1]èŒƒå›´çš„beliefå€¼")
        
        if anomalies:
            print("âš ï¸  å‘ç°å¼‚å¸¸:")
            for anomaly in anomalies:
                print(f"  - {anomaly}")
        else:
            print("âœ… æœªå‘ç°å¼‚å¸¸")
    
    def calculate_quality_score(self):
        """è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†"""
        print("\nâ­ 6. æ€»ä½“è´¨é‡è¯„åˆ†")
        print("-"*80)
        
        size = self.buffer.size
        if size == 0:
            print("æ— æ³•è®¡ç®—è¯„åˆ† - bufferä¸ºç©º")
            return
        
        score = 0
        max_score = 100
        
        # Scoring criteria
        rewards = self.buffer.reward[:size].flatten()
        states = self.buffer.state[:size]
        next_states = self.buffer.next_state[:size]
        
        # 1. Reward quality (30åˆ†)
        positive_ratio = np.sum(rewards > 0) / size
        score += min(30, positive_ratio * 30)
        
        # 2. State diversity (20åˆ†)
        beliefs = states[:, :5]
        unique_ratio = len(np.unique(beliefs, axis=0)) / size
        score += min(20, unique_ratio * 20)
        
        # 3. Learning effectiveness (30åˆ†)
        belief_changes = next_states[:, :5] - beliefs
        significant_changes = np.sum(np.abs(belief_changes).sum(axis=1) > 0.001) / size
        score += min(30, significant_changes * 30)
        
        # 4. Progress ratio (20åˆ†)
        target = np.array([0.7, 0.07, 0.07, 0.07, 0.08])
        current_distances = np.abs(beliefs - target).sum(axis=1)
        next_distances = np.abs(next_states[:, :5] - target).sum(axis=1)
        improvement_ratio = np.sum(current_distances > next_distances) / size
        score += min(20, improvement_ratio * 20)
        
        print(f"æ€»ä½“è¯„åˆ†: {score:.1f} / {max_score}")
        print(f"\nè¯„åˆ†ç»†èŠ‚:")
        print(f"  Rewardè´¨é‡:   {min(30, positive_ratio * 30):.1f} / 30")
        print(f"  Stateå¤šæ ·æ€§:  {min(20, unique_ratio * 20):.1f} / 20")
        print(f"  å­¦ä¹ æ•ˆæœ:     {min(30, significant_changes * 30):.1f} / 30")
        print(f"  Progress ratio:     {min(20, improvement_ratio * 20):.1f} / 20")
        
        # Rating
        if score >= 80:
            grade = "A (Excellent)"
            comment = "âœ… Bufferè´¨é‡å¾ˆå¥½ï¼ŒåŒ…å«é«˜è´¨é‡çš„å­¦ä¹ ç»éªŒ"
        elif score >= 60:
            grade = "B (Good)"
            comment = "âœ… Bufferè´¨é‡ä¸é”™ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ"
        elif score >= 40:
            grade = "C (Average)"
            comment = "âš ï¸  Bufferè´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥è®­ç»ƒç­–ç•¥"
        else:
            grade = "D (Poor)"
            comment = "âŒ Bufferè´¨é‡è¾ƒå·®ï¼Œéœ€è¦æ”¹è¿›è®­ç»ƒè¿‡ç¨‹"
        
        print(f"\nè¯„çº§: {grade}")
        print(f"{comment}")
    
    def save_analysis_report(self, filename="buffer_quality_report.json"):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        size = self.buffer.size
        if size == 0:
            return
        
        rewards = self.buffer.reward[:size].flatten()
        states = self.buffer.state[:size]
        next_states = self.buffer.next_state[:size]
        beliefs = states[:, :5]
        
        report = {
            "buffer_size": int(size),
            "buffer_capacity": int(self.buffer.max_size),
            "usage_ratio": float(size / self.buffer.max_size),
            "reward_stats": {
                "mean": float(rewards.mean()),
                "median": float(np.median(rewards)),
                "std": float(rewards.std()),
                "min": float(rewards.min()),
                "max": float(rewards.max()),
            },
            "reward_distribution": {
                "negative": int(np.sum(rewards < 0)),
                "zero": int(np.sum(rewards == 0)),
                "small_positive": int(np.sum((rewards > 0) & (rewards < 1))),
                "medium_positive": int(np.sum((rewards >= 1) & (rewards < 5))),
                "large_positive": int(np.sum(rewards >= 5)),
            },
            "belief_stats": {
                f"cluster_{i}": {
                    "mean": float(beliefs[:, i].mean()),
                    "std": float(beliefs[:, i].std()),
                    "min": float(beliefs[:, i].min()),
                    "max": float(beliefs[:, i].max()),
                }
                for i in range(5)
            },
        }
        
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨"""
    print("="*80)
    print("REPLAY BUFFER è´¨é‡åˆ†æå·¥å…·")
    print("="*80)
    
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("-"*80)
    print("""
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š

```python
from analyze_replay_buffer_quality import ReplayBufferAnalyzer

# åœ¨ recommendation_trainer.py ä¸­
def analyze_buffer_quality(self):
    analyzer = ReplayBufferAnalyzer(self.replay_buffer)
    analyzer.analyze_quality()
    analyzer.save_analysis_report()
```

æˆ–è€…åœ¨è®­ç»ƒçš„ç‰¹å®šæ—¶åˆ»è°ƒç”¨ï¼š

```python
# åœ¨ run_episode() ä¸­
if episode_num % 5 == 0:  # æ¯5ä¸ªepisodeåˆ†æä¸€æ¬¡
    print(f"\\nåˆ†æEpisode {episode_num}çš„Bufferè´¨é‡:")
    analyzer = ReplayBufferAnalyzer(self.replay_buffer)
    analyzer.analyze_quality()
```
    """)
    
    print("\nå»ºè®®çš„åˆ†ææ—¶æœº:")
    print("-"*80)
    print("1. è®­ç»ƒå¼€å§‹åç¬¬1ä¸ªepisode - æ£€æŸ¥åˆå§‹æ•°æ®è´¨é‡")
    print("2. æ¯5-10ä¸ªepisodes - ç›‘æ§å­¦ä¹ è¿›åº¦")
    print("3. è®­ç»ƒç»“æŸæ—¶ - è¯„ä¼°æœ€ç»ˆbufferè´¨é‡")
    print("4. å‘ç°è®­ç»ƒé—®é¢˜æ—¶ - è¯Šæ–­åŸå› ")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    main()
